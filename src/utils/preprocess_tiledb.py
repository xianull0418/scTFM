import argparse
import logging
from pathlib import Path
import numpy as np
import pandas as pd
import scanpy as sc
import tiledb
from tqdm import tqdm
import warnings
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
import json
import gc
import scipy.sparse as sp
from typing import List, Dict, Optional, Tuple
import threading
import queue
import time

import shutil
import subprocess

# å¿½ç•¥ Scanpy çš„ä¸€äº› FutureWarning
warnings.filterwarnings("ignore")

# è®¾ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_gene_vocab(vocab_path: str) -> List[str]:
    """åŠ è½½åŸºå› è¯è¡¨"""
    path = Path(vocab_path)
    if not path.exists():
        raise FileNotFoundError(f"Vocabulary file not found: {path}")
    with open(path, 'r') as f:
        genes = [line.strip() for line in f if line.strip()]
    return genes

def process_h5ad_vectorized(args) -> Optional[Dict]:
    """
    Worker å‡½æ•°ï¼šå¤„ç†å•ä¸ª h5ad æ–‡ä»¶
    """
    # ã€ä¼˜åŒ–ã€‘æ¥æ”¶é¢„æ„å»ºå¥½çš„ gene_mapï¼Œè€Œä¸æ˜¯ list
    file_path, target_gene_map, target_genes_list, min_genes, target_sum, is_ood_flag = args
    
    try:
        # 1. è¯»å–æ•°æ®
        adata = sc.read_h5ad(file_path)

        # 2. ç»Ÿä¸€åŸºå› åä¸ºç´¢å¼•
        if "gene_symbols" in adata.var.columns:
            adata.var_names = adata.var["gene_symbols"].astype(str)
        adata.var_names_make_unique()

        # 3. è¿‡æ»¤ç»†èƒ
        if min_genes > 0:
            sc.pp.filter_cells(adata, min_genes=min_genes)

        if adata.shape[0] == 0:
            return None

        # 4. å½’ä¸€åŒ– (æ³¨æ„ï¼šè¿™é‡Œä¼šæ”¹å˜æ•°æ®ä¸º log1p)
        sc.pp.normalize_total(adata, target_sum=target_sum)
        sc.pp.log1p(adata)

        # --- æ ¸å¿ƒå‘é‡åŒ–é€»è¾‘ ---
        
        # A. æ‰¾åˆ°äº¤é›†åŸºå›  (åˆ©ç”¨ set è½¬æ¢ list åŠ é€Ÿ isin)
        mask = np.isin(adata.var_names, target_genes_list)
        
        # B. åˆ‡ç‰‡
        adata_sub = adata[:, mask]
        
        if adata_sub.shape[1] == 0:
            return None 
            
        # C. è½¬ COO
        X_coo = adata_sub.X.tocoo()
        
        # D. ç´¢å¼•é‡æ˜ å°„
        sub_gene_names = adata_sub.var_names
        
        # ä½¿ç”¨ä¼ å…¥çš„ map è¿›è¡Œæ˜ å°„
        local_to_global = np.array([target_gene_map[g] for g in sub_gene_names], dtype=np.int64)
        new_gene_indices = local_to_global[X_coo.col]
        
        # æ„å»ºç»“æœ
        res = {
            'n_cells': adata.shape[0],
            'row_indices': X_coo.row.astype(np.int64),
            'col_indices': new_gene_indices,
            'values': X_coo.data.astype(np.float32),
            'is_ood': is_ood_flag,
            'file_path': str(file_path)
        }
        
        # ã€å®‰å…¨ã€‘ä¸»åŠ¨é‡Šæ”¾å¤§å¯¹è±¡å†…å­˜
        del adata, adata_sub, X_coo
        gc.collect()
        
        return res

    except Exception as e:
        logger.error(f"Error processing {file_path}: {e}")
        return None

def init_tiledb_array(output_dir: Path, n_genes: int):
    tiledb_path = output_dir / "all_data"
    
    if tiledb_path.exists():
        import shutil
        logger.warning(f"Output path exists, cleaning up: {tiledb_path}")
        shutil.rmtree(tiledb_path)
    tiledb_path.mkdir(parents=True)
    
    # ç“¦ç‰‡å¤§å° 4096 (scimilarity æ¨è: 2048-4096)
    # é¿å… GPFS ä¸Šçš„è¯»æ”¾å¤§
    tile_extent = 4096
    # é˜²æ­¢ int64 æº¢å‡º
    max_domain = np.iinfo(np.int64).max - tile_extent - 1000
    
    # é»˜è®¤å‹ç¼©è¿‡æ»¤å™¨
    filters = [tiledb.ZstdFilter(level=4)]
    
    # 1. Counts Schema (Sparse)
    counts_uri = str(tiledb_path / "counts")
    dom = tiledb.Domain(
        tiledb.Dim(name="cell_index", domain=(0, max_domain), tile=tile_extent, dtype=np.int64),
        tiledb.Dim(name="gene_index", domain=(0, n_genes - 1), tile=n_genes, dtype=np.int64),
    )
    schema = tiledb.ArraySchema(
        domain=dom, sparse=True, 
        attrs=[tiledb.Attr(name="data", dtype=np.float32, filters=filters)], 
        allows_duplicates=False,
        coords_filters=filters,
        offsets_filters=filters
    )
    tiledb.Array.create(counts_uri, schema)
    
    # 2. Metadata Schema (Dense!)
    # ä¼˜åŒ–ï¼šä½¿ç”¨ Dense Array å­˜å‚¨ Metadataï¼Œæ”¯æŒ O(1) éšæœºè®¿é—®
    meta_uri = str(tiledb_path / "cell_metadata")
    dom_meta = tiledb.Domain(
        tiledb.Dim(name="cell_index", domain=(0, max_domain), tile=tile_extent, dtype=np.int64)
    )
    schema_meta = tiledb.ArraySchema(
        domain=dom_meta, sparse=False, # Changed to Dense
        attrs=[
            tiledb.Attr(name="is_ood", dtype=np.int8, filters=filters),
            tiledb.Attr(name="file_source", dtype='ascii', var=True, filters=filters) 
        ]
    )
    tiledb.Array.create(meta_uri, schema_meta)
    
    return tiledb_path

class AsyncBatchWriter:
    def __init__(self, tiledb_path: Path, batch_size: int = 500000):
        self.counts_uri = str(tiledb_path / "counts")
        self.meta_uri = str(tiledb_path / "cell_metadata")
        self.batch_size = batch_size
        self.global_cell_offset = 0
        
        self.current_buffer = self._init_buffer()
        self.current_count = 0
        
        # ã€é‡è¦ä¿®å¤ã€‘è®¾ç½® maxsize é˜²æ­¢å†…å­˜çˆ†ç‚¸ (èƒŒå‹æœºåˆ¶)
        # å…è®¸é˜Ÿåˆ—é‡Œå­˜ 3 ä¸ª batchï¼Œå¦‚æœæ»¡äº†ï¼Œä¸»è¿›ç¨‹çš„ add() ä¼šé˜»å¡ç­‰å¾…
        self.write_queue = queue.Queue(maxsize=3)
        
        self.is_running = True
        self.writer_thread = threading.Thread(target=self._writer_loop, daemon=True)
        self.writer_thread.start()
        self.write_error = None

    def _init_buffer(self):
        return {
            'rows': [], 'cols': [], 'vals': [],
            'meta_indices': [], 'meta_ood': [], 'meta_src': []
        }

    def add(self, result: Dict):
        if self.write_error: raise self.write_error
        if not result: return
            
        n_cells = result['n_cells']
        global_rows = result['row_indices'] + self.global_cell_offset
        
        self.current_buffer['rows'].append(global_rows)
        self.current_buffer['cols'].append(result['col_indices'])
        self.current_buffer['vals'].append(result['values'])
        self.current_buffer['meta_indices'].append(np.arange(self.global_cell_offset, self.global_cell_offset + n_cells))
        self.current_buffer['meta_ood'].append(np.full(n_cells, result['is_ood'], dtype=np.int8))
        self.current_buffer['meta_src'].extend([result['file_path']] * n_cells)
        
        self.global_cell_offset += n_cells
        self.current_count += n_cells
        
        if self.current_count >= self.batch_size:
            self._push_to_queue()

    def _push_to_queue(self):
        if self.current_count == 0: return
        logger.info(f"  [Main] Batch full ({self.current_count} cells). Pushing to queue (Size: {self.write_queue.qsize()})...")
        
        task = (self.current_buffer, self.current_count)
        # put é»˜è®¤æ˜¯é˜»å¡çš„ï¼Œå¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œè¿™é‡Œä¼šåœä¸‹ç­‰å¾…ï¼Œä¿æŠ¤å†…å­˜
        self.write_queue.put(task)
        
        self.current_buffer = self._init_buffer()
        self.current_count = 0

    def wait_until_idle(self):
        """ç­‰å¾…æ‰€æœ‰å†™å…¥ä»»åŠ¡å®Œæˆ (ç”¨äºä¸­é—´åŒæ­¥)"""
        # 1. å¼ºåˆ¶æäº¤å½“å‰ç¼“å­˜ä¸­çš„å‰©ä½™æ•°æ®
        self._push_to_queue()
        # 2. é˜»å¡ç›´åˆ°é˜Ÿåˆ—æ¸…ç©º
        self.write_queue.join()
        if self.write_error: raise self.write_error

    def _writer_loop(self):
        while self.is_running or not self.write_queue.empty():
            try:
                try:
                    buffer, count = self.write_queue.get(timeout=1)
                except queue.Empty:
                    continue
                
                # --- [æ–°å¢] è¯¦ç»†åˆ†æ®µè®¡æ—¶ ---
                logger.info(f"  [Debug] Start writing batch of {count} cells...")
                t_start = time.time()
                
                # 1. å†…å­˜æ‹¼æ¥ (æ£€æµ‹æ˜¯å¦çˆ†å†…å­˜/Swap)
                all_rows = np.concatenate(buffer['rows'])
                all_cols = np.concatenate(buffer['cols'])
                all_vals = np.concatenate(buffer['vals'])
                
                t_concat = time.time()
                logger.info(f"  [Debug] Step 1: Numpy Concat used {t_concat - t_start:.2f}s") # å¦‚æœè¿™é‡Œæ…¢ï¼Œè¯´æ˜å†…å­˜çˆ†äº†

                # 2. TileDB ä¸Šä¸‹æ–‡é…ç½®
                cfg = tiledb.Config({
                    "sm.compute_concurrency_level": "100",
                    "sm.io_concurrency_level": "16",
                    "vfs.file.enable_filelocks": "false", # ç¡®ä¿é”å·²å…³
                })
                ctx = tiledb.Ctx(cfg)
                
                # 3. å†™ Counts (æ£€æµ‹ç¡¬ç›˜é€Ÿåº¦)
                with tiledb.open(self.counts_uri, 'w', ctx=ctx) as arr:
                    arr[all_rows, all_cols] = all_vals
                
                t_write_counts = time.time()
                logger.info(f"  [Debug] Step 2: Write Counts Disk IO used {t_write_counts - t_concat:.2f}s") # å¦‚æœè¿™é‡Œæ…¢ï¼Œè¯´æ˜æ˜¯ GPFS çš„é—®é¢˜

                # 4. å†™ Metadata (Dense Optimized)
                all_meta_indices = np.concatenate(buffer['meta_indices'])
                all_meta_ood = np.concatenate(buffer['meta_ood'])
                
                # Dense Array å†™å…¥ä¼˜åŒ–ï¼šä½¿ç”¨åˆ‡ç‰‡èµ‹å€¼
                start_idx = int(all_meta_indices[0])
                end_idx = int(all_meta_indices[-1]) + 1
                
                with tiledb.open(self.meta_uri, 'w', ctx=ctx) as arr:
                    arr[start_idx:end_idx] = {
                        'is_ood': all_meta_ood,
                        'file_source': np.array(buffer['meta_src'], dtype=object)
                    }
                
                # 5. æ¸…ç†
                del buffer, all_rows, all_cols, all_vals
                gc.collect()
                
                logger.info(f"  [Async] Total Batch Time: {time.time()-t_start:.1f}s.")
                self.write_queue.task_done()
                
            except Exception as e:
                logger.error(f"Async Writer Crashed: {e}")
                self.write_error = e
                break

    def finish(self):
        self._push_to_queue()
        self.is_running = False
        self.writer_thread.join()
        if self.write_error: raise self.write_error

def consolidate_arrays(tiledb_path: Path):
    """åˆå¹¶ TileDB ç¢ç‰‡ä»¥ä¼˜åŒ–è¯»å–æ€§èƒ½"""
    logger.info(f"Starting consolidation on {tiledb_path}...")
    
    # é™åˆ¶åˆå¹¶æ—¶çš„å†…å­˜ä½¿ç”¨ï¼Œé˜²æ­¢ç‚¸æœº
    cfg = tiledb.Config({
        "sm.consolidation.buffer_size": "2147483648",  # 2GB buffer limit
        "sm.compute_concurrency_level": "16"
    })
    ctx = tiledb.Ctx(cfg)
    
    try:
        # 1. Consolidate Counts
        counts_uri = str(tiledb_path / "counts")
        tiledb.consolidate(counts_uri, ctx=ctx)
        tiledb.vacuum(counts_uri, ctx=ctx)
        
        # 2. Consolidate Metadata
        meta_uri = str(tiledb_path / "cell_metadata")
        tiledb.consolidate(meta_uri, ctx=ctx)
        tiledb.vacuum(meta_uri, ctx=ctx)
        
        logger.info("Consolidation complete.")
    except Exception as e:
        logger.error(f"Consolidation failed (non-critical): {e}")

def offload_batch_to_disk(shm_path: Path, final_path: Path):
    """
    ã€æ ¸å¿ƒä¼˜åŒ–ã€‘å†…å­˜å¸è½½æœºåˆ¶ï¼š
    1. å°† RAM ä¸­çš„æ–°æ•°æ®ç¢ç‰‡ Rsync åˆ°ç¡¬ç›˜ (Append æ¨¡å¼)
    2. åˆ é™¤ RAM ä¸­å·²åŒæ­¥çš„ç¢ç‰‡ï¼Œé‡Šæ”¾å†…å­˜
    è¿™ä½¿å¾— RAM å˜æˆä¸€ä¸ªæ— é™å¾ªç¯ä½¿ç”¨çš„æ»‘åŠ¨çª—å£ç¼“å†²ã€‚
    """
    logger.info("ğŸ”„ Offloading RAM buffer to Disk (Freeing Memory)...")
    t_start = time.time()
    
    # 1. Rsync (RAM -> GPFS) - Append Mode (ä¸¥ç¦ä½¿ç”¨ --delete)
    # è¿™ä¼šå°†æ–°ç”Ÿæˆçš„ fragments å¤åˆ¶åˆ°ç¡¬ç›˜
    if not final_path.parent.exists():
        final_path.parent.mkdir(parents=True, exist_ok=True)
        
    # æ³¨æ„ï¼šæºç›®å½•åŠ æ–œæ  (shm_path/) è¡¨ç¤ºåŒæ­¥å†…å®¹
    # -a: å½’æ¡£æ¨¡å¼ (é€’å½’ + ä¿ç•™å±æ€§)
    cmd = ["rsync", "-av", str(shm_path) + "/", str(final_path) + "/"]
    try:
        subprocess.run(cmd, check=True)
    except Exception as e:
        logger.error(f"âŒ Sync failed: {e}")
        return

    # 2. æ¸…ç† RAM ä¸­çš„ç¢ç‰‡ (é‡Šæ”¾å†…å­˜)
    # æˆ‘ä»¬åªåˆ é™¤ __fragments ä¸‹çš„æ•°æ®ï¼Œä¿ç•™ Schema å’Œå…ƒæ•°æ®ç»“æ„
    tiledb_root = shm_path / "all_data"
    total_cleaned = 0
    
    if tiledb_root.exists():
        for array_dir in tiledb_root.iterdir():
            if not array_dir.is_dir(): continue
            
            frag_dir = array_dir / "__fragments"
            if frag_dir.exists():
                for frag in frag_dir.iterdir():
                    # ç¢ç‰‡é€šå¸¸æ˜¯ç›®å½• (uuid_timestamp_...)
                    if frag.is_dir(): 
                        shutil.rmtree(frag)
                        total_cleaned += 1
                    
    logger.info(f"âœ… Offload complete. Cleaned {total_cleaned} fragments from RAM. Time: {time.time() - t_start:.1f}s")

def main():
    parser = argparse.ArgumentParser(description="Efficient TileDB Converter (In-Memory Fast Track)")
    parser.add_argument("--csv_path", type=str, default="data/assets/ae_data_info.csv")
    parser.add_argument("--vocab_path", type=str, default="data/assets/gene_order.tsv")
    
    # ã€æœ€ç»ˆç›®çš„åœ°ã€‘GPFS è·¯å¾„
    parser.add_argument("--final_output_dir", type=str, 
                        default="/gpfs/hybrid/data/downloads/gcloud/arc-scbasecount/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/tiledb_100m")
    
    parser.add_argument("--min_genes", type=int, default=200)
    parser.add_argument("--target_sum", type=float, default=1e4)
    parser.add_argument("--num_workers", type=int, default=64) # ä¿æŒ 64 ä»¥é˜² OOM
    # ä¼˜åŒ–ï¼šè®¾ç½®ä¸º Tile Extent (4096) çš„æ•´æ•°å€ï¼Œç¡®ä¿ Dense Array å†™å…¥å¯¹é½
    # 4096 * 128 = 524288
    parser.add_argument("--batch_size", type=int, default=524288)
    parser.add_argument("--max_files", type=int, default=-1, help="å¤„ç†æ–‡ä»¶æ•°é‡ (-1 è¡¨ç¤ºå¤„ç† CSV ä¸­çš„æ‰€æœ‰æ–‡ä»¶)")
    parser.add_argument("--sync_interval", type=int, default=3100, help="æ¯å¤„ç†å¤šå°‘ä¸ªæ–‡ä»¶å¸è½½ä¸€æ¬¡å†…å­˜åˆ°ç¡¬ç›˜ (å»ºè®® 200-500)")
    
    args = parser.parse_args()
    
    # --- 1. è®¾ç½®æé€Ÿå†…å­˜è·¯å¾„ ---
    # åˆ©ç”¨ Linux çš„ /dev/shm (Shared Memory)ï¼Œé€Ÿåº¦æ¯” NVMe å¿« 10 å€ï¼Œä¸”æ²¡æœ‰æ–‡ä»¶é”å»¶è¿Ÿ
    shm_path = Path("/dev/shm/tiledb_fast_buffer")
    final_path = Path(args.final_output_dir)
    
    logger.info("="*60)
    logger.info(f"ğŸš€ SPEED MODE ACTIVATED")
    logger.info(f"1. Working Directory (RAM): {shm_path}")
    logger.info(f"2. Final Destination (SSD): {final_path}")
    logger.info("="*60)
    
    # æ¸…ç†æ—§çš„å†…å­˜ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if shm_path.exists():
        logger.warning(f"Cleaning up previous buffer at {shm_path}...")
        shutil.rmtree(shm_path)
    
    # 2. å‡†å¤‡å·¥ä½œ
    target_genes = load_gene_vocab(args.vocab_path)
    target_gene_map = {g: i for i, g in enumerate(target_genes)}
    
    info_df = pd.read_csv(args.csv_path)
    if args.max_files > 0:
        info_df = info_df.head(args.max_files)

    # 3. åˆå§‹åŒ– TileDB (åœ¨å†…å­˜ç›˜ä¸Šï¼)
    tiledb_path = init_tiledb_array(shm_path, len(target_genes))
    
    # å†™å…¥åŸºå› æ³¨é‡Š
    gene_annot_uri = str(tiledb_path / "gene_annotation")
    tiledb.Array.create(gene_annot_uri, tiledb.ArraySchema(
        domain=tiledb.Domain(tiledb.Dim(name="gene_index", domain=(0, len(target_genes)-1), tile=len(target_genes), dtype=np.int64)),
        sparse=False,
        attrs=[tiledb.Attr(name="gene_symbol", dtype='ascii', var=True)]
    ))
    with tiledb.open(gene_annot_uri, 'w') as arr:
        arr[:] = {'gene_symbol': np.array(target_genes, dtype=object)}

    # 4. å‡†å¤‡ä»»åŠ¡
    tasks = []
    for _, row in info_df.iterrows():
        is_ood = int(row.get('full_validation_dataset', 0))
        tasks.append((
            row['file_path'], target_gene_map, target_genes,
            args.min_genes, args.target_sum, is_ood
        ))

    # 5. æ‰§è¡Œ (å†™å…¥å†…å­˜ç›˜)
    writer = AsyncBatchWriter(tiledb_path, batch_size=args.batch_size)
    logger.info(f"Starting processing {len(tasks)} files...")
    
    processed_count = 0
    sync_interval = args.sync_interval

    with ProcessPoolExecutor(max_workers=args.num_workers) as executor:
        futures = {executor.submit(process_h5ad_vectorized, task): task[0] for task in tasks}
        for future in tqdm(as_completed(futures), total=len(tasks), desc="Processing & Writing (RAM)"):
            try:
                result = future.result()
                if result: 
                    writer.add(result)
                    
                    # --- [æ–°å¢] å®šæœŸåŒæ­¥é€»è¾‘ ---
                    processed_count += 1
                    if sync_interval > 0 and processed_count % sync_interval == 0:
                        logger.info(f"â³ Reached {processed_count} files. Pausing to offload RAM...")
                        writer.wait_until_idle() # 1. ç¡®ä¿å†™å…¥é˜Ÿåˆ—æ¸…ç©º
                        offload_batch_to_disk(shm_path, final_path) # 2. å¸è½½æ•°æ®å¹¶æ¸…ç† RAM
                        logger.info("â–¶ï¸ Resuming processing...")
                        
            except Exception as e:
                logger.error(f"Failed: {e}")

    writer.finish()
    
    # --- [ä¿®æ”¹] ä¿å­˜ Metadata (æå‰åˆ°æ¬è¿å‰) ---
    metadata = {
        'total_cells': writer.global_cell_offset,
        'n_genes': len(target_genes),
        'storage_path': str(final_path)
    }
    with open(tiledb_path / 'metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)

    # --- [ä¿®æ”¹] 6. æœ€ç»ˆæ¬è¿ (RAM -> GPFS) ---
    # ç­–ç•¥ï¼šå…ˆæ¬è¿ï¼Œé‡Šæ”¾ RAMï¼Œå†åšåˆå¹¶ï¼
    logger.info("="*60)
    logger.info("Processing Complete. Finalizing data move...")
    logger.info("Strategy: Move remaining fragments -> Clear RAM -> Consolidate on Disk")
    
    # ç¡®ä¿ç›®æ ‡çˆ¶ç›®å½•å­˜åœ¨
    final_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨ rsync è¿›è¡Œæ¬è¿
    try:
        # [å…³é”®ä¿®æ”¹] å»æ‰ --deleteï¼Œå› ä¸ºä¹‹å‰å·²ç»å¸è½½äº†ä¸€éƒ¨åˆ†æ•°æ®åˆ°ç¡¬ç›˜ï¼Œ
        # å¦‚æœä½¿ç”¨ deleteï¼Œä¼šæŠŠä¹‹å‰å¸è½½çš„æ•°æ®ï¼ˆå› ä¸ºä¸åœ¨å½“å‰çš„ RAM é‡Œï¼‰ç»™åˆ æ‰ï¼
        cmd = ["rsync", "-avP", str(shm_path) + "/", str(final_path) + "/"]
        subprocess.run(cmd, check=True)
        logger.info(f"âœ… SUCCESS! Final data moved to: {final_path}")
        
        # æ¬è¿æˆåŠŸåï¼Œæ¸…ç†å†…å­˜ï¼Œé˜²æ­¢ OOM
        shutil.rmtree(shm_path)
        logger.info("RAM buffer cleaned. Memory released.")
        
        # --- [ä¿®æ”¹] 7. åœ¨ç¡¬ç›˜ä¸Šåˆå¹¶ (Consolidation) ---
        # ç°åœ¨å†…å­˜ç©ºå‡ºæ¥äº†ï¼Œå¯ä»¥å®‰å…¨åœ°åœ¨ GPFS ä¸Šåšåˆå¹¶
        # æ³¨æ„ï¼šè·¯å¾„è¦æŒ‡å‘ final_path ä¸‹çš„ all_data
        target_tiledb_path = final_path / "all_data"
        consolidate_arrays(target_tiledb_path)
        
    except Exception as e:
        logger.error(f"âŒ Error during move/consolidate: {e}")
        logger.error(f"âš ï¸ Check {shm_path} or {final_path}")

if __name__ == "__main__":
    multiprocessing.set_start_method('fork', force=True)
    main()
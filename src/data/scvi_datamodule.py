"""
scVI-style Single-Cell DataModule

ä½¿ç”¨ SomaSCVIDatasetï¼Œè¿”å›:
- x: log1p(normalized) - Encoder è¾“å…¥
- counts: normalized counts (æœª log) - NB Loss ç›®æ ‡
- library_size: æ¯ä¸ªç»†èƒçš„ UMI æ€»æ•° - Decoder ç¼©æ”¾
"""

from typing import Optional, Dict
import os

import torch
from pytorch_lightning import LightningDataModule
from torch.utils.data import DataLoader

from src.data.components.scvi_dataset import SomaSCVIDataset


class SCVIDataModule(LightningDataModule):
    """
    scVI-style å•ç»†èƒæ•°æ® DataModule

    ä¸ SingleCellDataModule çš„åŒºåˆ«:
    - ä½¿ç”¨ SomaSCVIDataset (è¿”å› dict è€Œé tuple)
    - åŒ…å« counts å’Œ library_size ç”¨äº NB loss

    Split Labels:
    0: Train (ID) - ç”¨äºè®­ç»ƒ
    1: Val (ID)   - ç”¨äºéªŒè¯
    2: Test (ID)  - ç”¨äºæµ‹è¯• (åŒåˆ†å¸ƒ)
    3: Test (OOD) - ç”¨äºæµ‹è¯• (å¤–åˆ†å¸ƒ)
    """

    def __init__(
        self,
        data_dir: str = "data/",
        batch_size: int = 256,
        num_workers: int = 4,
        pin_memory: bool = True,
        io_chunk_size: int = 16384,
        prefetch_factor: int = 2,
        persistent_workers: bool = True,
        shard_assignment: Optional[Dict] = None,
        use_counts_layer: bool = False,  # é»˜è®¤ Falseï¼Œç”¨ expm1(X) æ›´å¿«
    ):
        """
        Args:
            data_dir: æ•°æ®é›†æ ¹ç›®å½• (scVI é¢„å¤„ç†åçš„ TileDB ç›®å½•)
            batch_size: æ¯ä¸ª batch çš„å¤§å°
            num_workers: DataLoader çš„ worker æ•°é‡
            pin_memory: æ˜¯å¦å°†æ•°æ®é”åœ¨å†…å­˜ä¸­
            io_chunk_size: TileDB è¯»å–æ—¶çš„ chunk å¤§å°
            prefetch_factor: æ¯ä¸ª worker é¢„åŠ è½½çš„ batch æ•°é‡
            persistent_workers: æ˜¯å¦ä¿æŒ workers å­˜æ´»
            shard_assignment: æ™ºèƒ½è´Ÿè½½å‡è¡¡çš„ shard åˆ†é…æ–¹æ¡ˆ
            use_counts_layer: æ˜¯å¦ä» counts layer è¯»å– (False ç”¨ expm1 è®¡ç®—ï¼Œæ›´å¿«)
        """
        super().__init__()

        self.save_hyperparameters(logger=False)

        self.data_train: Optional[SomaSCVIDataset] = None
        self.data_val: Optional[SomaSCVIDataset] = None

        self._cached_sub_uris: Optional[list] = None

    def setup(self, stage: Optional[str] = None):
        """åŠ è½½æ•°æ®é›†"""
        if not self.data_train and not self.data_val:
            # é¢„æ‰«æ Shards
            if self._cached_sub_uris is None:
                print(f"ğŸ” [SCVIDataModule] Pre-scanning shards in {self.hparams.data_dir}...")
                self._cached_sub_uris = sorted([
                    os.path.join(self.hparams.data_dir, d)
                    for d in os.listdir(self.hparams.data_dir)
                    if os.path.isdir(os.path.join(self.hparams.data_dir, d))
                ])
                print(f"âœ… [SCVIDataModule] Found {len(self._cached_sub_uris)} shards")

            # è®­ç»ƒé›†
            self.data_train = SomaSCVIDataset(
                root_dir=self.hparams.data_dir,
                split_label=0,
                io_chunk_size=self.hparams.io_chunk_size,
                batch_size=self.hparams.batch_size,
                preloaded_sub_uris=self._cached_sub_uris,
                shard_assignment=self.hparams.shard_assignment,
                use_counts_layer=self.hparams.use_counts_layer,
            )

            # éªŒè¯é›†
            self.data_val = SomaSCVIDataset(
                root_dir=self.hparams.data_dir,
                split_label=1,
                io_chunk_size=self.hparams.io_chunk_size,
                batch_size=self.hparams.batch_size,
                preloaded_sub_uris=self._cached_sub_uris,
                shard_assignment=None,
                use_counts_layer=self.hparams.use_counts_layer,
            )

    def train_dataloader(self):
        """è¿”å›è®­ç»ƒé›†çš„ DataLoader"""
        return DataLoader(
            dataset=self.data_train,
            batch_size=None,  # Dataset å·²ç»å¤„ç†äº† batching
            num_workers=self.hparams.num_workers,
            prefetch_factor=self.hparams.prefetch_factor,
            pin_memory=self.hparams.pin_memory,
            persistent_workers=self.hparams.persistent_workers,
        )

    def val_dataloader(self):
        """è¿”å›éªŒè¯é›†çš„ DataLoader"""
        return DataLoader(
            dataset=self.data_val,
            batch_size=None,
            num_workers=self.hparams.num_workers,
            prefetch_factor=self.hparams.prefetch_factor,
            pin_memory=self.hparams.pin_memory,
            persistent_workers=self.hparams.persistent_workers,
        )

    def teardown(self, stage: Optional[str] = None):
        pass

    def state_dict(self):
        return {}

    def load_state_dict(self, state_dict):
        pass
